{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"D:/gitcode/DataScience/RMxnet/inception-bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward.load(\"Inception-BN\",126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "internels = model.symbol.get_internals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'conv_1_weight',\n",
       " 'conv_1_bias',\n",
       " 'conv_1_output',\n",
       " 'bn_1_gamma',\n",
       " 'bn_1_beta',\n",
       " 'bn_1_output',\n",
       " 'relu_1_output',\n",
       " 'pool_1_output',\n",
       " 'conv_2_red_weight',\n",
       " 'conv_2_red_bias',\n",
       " 'conv_2_red_output',\n",
       " 'bn_2_red_gamma',\n",
       " 'bn_2_red_beta',\n",
       " 'bn_2_red_output',\n",
       " 'relu_2_red_output',\n",
       " 'conv_2_weight',\n",
       " 'conv_2_bias',\n",
       " 'conv_2_output',\n",
       " 'bn_2_gamma',\n",
       " 'bn_2_beta',\n",
       " 'bn_2_output',\n",
       " 'relu_2_output',\n",
       " 'pool_2_output',\n",
       " 'conv_3a_1x1_weight',\n",
       " 'conv_3a_1x1_bias',\n",
       " 'conv_3a_1x1_output',\n",
       " 'bn_3a_1x1_gamma',\n",
       " 'bn_3a_1x1_beta',\n",
       " 'bn_3a_1x1_output',\n",
       " 'relu_3a_1x1_output',\n",
       " 'conv_3a_3x3_reduce_weight',\n",
       " 'conv_3a_3x3_reduce_bias',\n",
       " 'conv_3a_3x3_reduce_output',\n",
       " 'bn_3a_3x3_reduce_gamma',\n",
       " 'bn_3a_3x3_reduce_beta',\n",
       " 'bn_3a_3x3_reduce_output',\n",
       " 'relu_3a_3x3_reduce_output',\n",
       " 'conv_3a_3x3_weight',\n",
       " 'conv_3a_3x3_bias',\n",
       " 'conv_3a_3x3_output',\n",
       " 'bn_3a_3x3_gamma',\n",
       " 'bn_3a_3x3_beta',\n",
       " 'bn_3a_3x3_output',\n",
       " 'relu_3a_3x3_output',\n",
       " 'conv_3a_double_3x3_reduce_weight',\n",
       " 'conv_3a_double_3x3_reduce_bias',\n",
       " 'conv_3a_double_3x3_reduce_output',\n",
       " 'bn_3a_double_3x3_reduce_gamma',\n",
       " 'bn_3a_double_3x3_reduce_beta',\n",
       " 'bn_3a_double_3x3_reduce_output',\n",
       " 'relu_3a_double_3x3_reduce_output',\n",
       " 'conv_3a_double_3x3_0_weight',\n",
       " 'conv_3a_double_3x3_0_bias',\n",
       " 'conv_3a_double_3x3_0_output',\n",
       " 'bn_3a_double_3x3_0_gamma',\n",
       " 'bn_3a_double_3x3_0_beta',\n",
       " 'bn_3a_double_3x3_0_output',\n",
       " 'relu_3a_double_3x3_0_output',\n",
       " 'conv_3a_double_3x3_1_weight',\n",
       " 'conv_3a_double_3x3_1_bias',\n",
       " 'conv_3a_double_3x3_1_output',\n",
       " 'bn_3a_double_3x3_1_gamma',\n",
       " 'bn_3a_double_3x3_1_beta',\n",
       " 'bn_3a_double_3x3_1_output',\n",
       " 'relu_3a_double_3x3_1_output',\n",
       " 'avg_pool_3a_pool_output',\n",
       " 'conv_3a_proj_weight',\n",
       " 'conv_3a_proj_bias',\n",
       " 'conv_3a_proj_output',\n",
       " 'bn_3a_proj_gamma',\n",
       " 'bn_3a_proj_beta',\n",
       " 'bn_3a_proj_output',\n",
       " 'relu_3a_proj_output',\n",
       " 'ch_concat_3a_chconcat_output',\n",
       " 'conv_3b_1x1_weight',\n",
       " 'conv_3b_1x1_bias',\n",
       " 'conv_3b_1x1_output',\n",
       " 'bn_3b_1x1_gamma',\n",
       " 'bn_3b_1x1_beta',\n",
       " 'bn_3b_1x1_output',\n",
       " 'relu_3b_1x1_output',\n",
       " 'conv_3b_3x3_reduce_weight',\n",
       " 'conv_3b_3x3_reduce_bias',\n",
       " 'conv_3b_3x3_reduce_output',\n",
       " 'bn_3b_3x3_reduce_gamma',\n",
       " 'bn_3b_3x3_reduce_beta',\n",
       " 'bn_3b_3x3_reduce_output',\n",
       " 'relu_3b_3x3_reduce_output',\n",
       " 'conv_3b_3x3_weight',\n",
       " 'conv_3b_3x3_bias',\n",
       " 'conv_3b_3x3_output',\n",
       " 'bn_3b_3x3_gamma',\n",
       " 'bn_3b_3x3_beta',\n",
       " 'bn_3b_3x3_output',\n",
       " 'relu_3b_3x3_output',\n",
       " 'conv_3b_double_3x3_reduce_weight',\n",
       " 'conv_3b_double_3x3_reduce_bias',\n",
       " 'conv_3b_double_3x3_reduce_output',\n",
       " 'bn_3b_double_3x3_reduce_gamma',\n",
       " 'bn_3b_double_3x3_reduce_beta',\n",
       " 'bn_3b_double_3x3_reduce_output',\n",
       " 'relu_3b_double_3x3_reduce_output',\n",
       " 'conv_3b_double_3x3_0_weight',\n",
       " 'conv_3b_double_3x3_0_bias',\n",
       " 'conv_3b_double_3x3_0_output',\n",
       " 'bn_3b_double_3x3_0_gamma',\n",
       " 'bn_3b_double_3x3_0_beta',\n",
       " 'bn_3b_double_3x3_0_output',\n",
       " 'relu_3b_double_3x3_0_output',\n",
       " 'conv_3b_double_3x3_1_weight',\n",
       " 'conv_3b_double_3x3_1_bias',\n",
       " 'conv_3b_double_3x3_1_output',\n",
       " 'bn_3b_double_3x3_1_gamma',\n",
       " 'bn_3b_double_3x3_1_beta',\n",
       " 'bn_3b_double_3x3_1_output',\n",
       " 'relu_3b_double_3x3_1_output',\n",
       " 'avg_pool_3b_pool_output',\n",
       " 'conv_3b_proj_weight',\n",
       " 'conv_3b_proj_bias',\n",
       " 'conv_3b_proj_output',\n",
       " 'bn_3b_proj_gamma',\n",
       " 'bn_3b_proj_beta',\n",
       " 'bn_3b_proj_output',\n",
       " 'relu_3b_proj_output',\n",
       " 'ch_concat_3b_chconcat_output',\n",
       " 'conv_3c_3x3_reduce_weight',\n",
       " 'conv_3c_3x3_reduce_bias',\n",
       " 'conv_3c_3x3_reduce_output',\n",
       " 'bn_3c_3x3_reduce_gamma',\n",
       " 'bn_3c_3x3_reduce_beta',\n",
       " 'bn_3c_3x3_reduce_output',\n",
       " 'relu_3c_3x3_reduce_output',\n",
       " 'conv_3c_3x3_weight',\n",
       " 'conv_3c_3x3_bias',\n",
       " 'conv_3c_3x3_output',\n",
       " 'bn_3c_3x3_gamma',\n",
       " 'bn_3c_3x3_beta',\n",
       " 'bn_3c_3x3_output',\n",
       " 'relu_3c_3x3_output',\n",
       " 'conv_3c_double_3x3_reduce_weight',\n",
       " 'conv_3c_double_3x3_reduce_bias',\n",
       " 'conv_3c_double_3x3_reduce_output',\n",
       " 'bn_3c_double_3x3_reduce_gamma',\n",
       " 'bn_3c_double_3x3_reduce_beta',\n",
       " 'bn_3c_double_3x3_reduce_output',\n",
       " 'relu_3c_double_3x3_reduce_output',\n",
       " 'conv_3c_double_3x3_0_weight',\n",
       " 'conv_3c_double_3x3_0_bias',\n",
       " 'conv_3c_double_3x3_0_output',\n",
       " 'bn_3c_double_3x3_0_gamma',\n",
       " 'bn_3c_double_3x3_0_beta',\n",
       " 'bn_3c_double_3x3_0_output',\n",
       " 'relu_3c_double_3x3_0_output',\n",
       " 'conv_3c_double_3x3_1_weight',\n",
       " 'conv_3c_double_3x3_1_bias',\n",
       " 'conv_3c_double_3x3_1_output',\n",
       " 'bn_3c_double_3x3_1_gamma',\n",
       " 'bn_3c_double_3x3_1_beta',\n",
       " 'bn_3c_double_3x3_1_output',\n",
       " 'relu_3c_double_3x3_1_output',\n",
       " 'max_pool_3c_pool_output',\n",
       " 'ch_concat_3c_chconcat_output',\n",
       " 'conv_4a_1x1_weight',\n",
       " 'conv_4a_1x1_bias',\n",
       " 'conv_4a_1x1_output',\n",
       " 'bn_4a_1x1_gamma',\n",
       " 'bn_4a_1x1_beta',\n",
       " 'bn_4a_1x1_output',\n",
       " 'relu_4a_1x1_output',\n",
       " 'conv_4a_3x3_reduce_weight',\n",
       " 'conv_4a_3x3_reduce_bias',\n",
       " 'conv_4a_3x3_reduce_output',\n",
       " 'bn_4a_3x3_reduce_gamma',\n",
       " 'bn_4a_3x3_reduce_beta',\n",
       " 'bn_4a_3x3_reduce_output',\n",
       " 'relu_4a_3x3_reduce_output',\n",
       " 'conv_4a_3x3_weight',\n",
       " 'conv_4a_3x3_bias',\n",
       " 'conv_4a_3x3_output',\n",
       " 'bn_4a_3x3_gamma',\n",
       " 'bn_4a_3x3_beta',\n",
       " 'bn_4a_3x3_output',\n",
       " 'relu_4a_3x3_output',\n",
       " 'conv_4a_double_3x3_reduce_weight',\n",
       " 'conv_4a_double_3x3_reduce_bias',\n",
       " 'conv_4a_double_3x3_reduce_output',\n",
       " 'bn_4a_double_3x3_reduce_gamma',\n",
       " 'bn_4a_double_3x3_reduce_beta',\n",
       " 'bn_4a_double_3x3_reduce_output',\n",
       " 'relu_4a_double_3x3_reduce_output',\n",
       " 'conv_4a_double_3x3_0_weight',\n",
       " 'conv_4a_double_3x3_0_bias',\n",
       " 'conv_4a_double_3x3_0_output',\n",
       " 'bn_4a_double_3x3_0_gamma',\n",
       " 'bn_4a_double_3x3_0_beta',\n",
       " 'bn_4a_double_3x3_0_output',\n",
       " 'relu_4a_double_3x3_0_output',\n",
       " 'conv_4a_double_3x3_1_weight',\n",
       " 'conv_4a_double_3x3_1_bias',\n",
       " 'conv_4a_double_3x3_1_output',\n",
       " 'bn_4a_double_3x3_1_gamma',\n",
       " 'bn_4a_double_3x3_1_beta',\n",
       " 'bn_4a_double_3x3_1_output',\n",
       " 'relu_4a_double_3x3_1_output',\n",
       " 'avg_pool_4a_pool_output',\n",
       " 'conv_4a_proj_weight',\n",
       " 'conv_4a_proj_bias',\n",
       " 'conv_4a_proj_output',\n",
       " 'bn_4a_proj_gamma',\n",
       " 'bn_4a_proj_beta',\n",
       " 'bn_4a_proj_output',\n",
       " 'relu_4a_proj_output',\n",
       " 'ch_concat_4a_chconcat_output',\n",
       " 'conv_4b_1x1_weight',\n",
       " 'conv_4b_1x1_bias',\n",
       " 'conv_4b_1x1_output',\n",
       " 'bn_4b_1x1_gamma',\n",
       " 'bn_4b_1x1_beta',\n",
       " 'bn_4b_1x1_output',\n",
       " 'relu_4b_1x1_output',\n",
       " 'conv_4b_3x3_reduce_weight',\n",
       " 'conv_4b_3x3_reduce_bias',\n",
       " 'conv_4b_3x3_reduce_output',\n",
       " 'bn_4b_3x3_reduce_gamma',\n",
       " 'bn_4b_3x3_reduce_beta',\n",
       " 'bn_4b_3x3_reduce_output',\n",
       " 'relu_4b_3x3_reduce_output',\n",
       " 'conv_4b_3x3_weight',\n",
       " 'conv_4b_3x3_bias',\n",
       " 'conv_4b_3x3_output',\n",
       " 'bn_4b_3x3_gamma',\n",
       " 'bn_4b_3x3_beta',\n",
       " 'bn_4b_3x3_output',\n",
       " 'relu_4b_3x3_output',\n",
       " 'conv_4b_double_3x3_reduce_weight',\n",
       " 'conv_4b_double_3x3_reduce_bias',\n",
       " 'conv_4b_double_3x3_reduce_output',\n",
       " 'bn_4b_double_3x3_reduce_gamma',\n",
       " 'bn_4b_double_3x3_reduce_beta',\n",
       " 'bn_4b_double_3x3_reduce_output',\n",
       " 'relu_4b_double_3x3_reduce_output',\n",
       " 'conv_4b_double_3x3_0_weight',\n",
       " 'conv_4b_double_3x3_0_bias',\n",
       " 'conv_4b_double_3x3_0_output',\n",
       " 'bn_4b_double_3x3_0_gamma',\n",
       " 'bn_4b_double_3x3_0_beta',\n",
       " 'bn_4b_double_3x3_0_output',\n",
       " 'relu_4b_double_3x3_0_output',\n",
       " 'conv_4b_double_3x3_1_weight',\n",
       " 'conv_4b_double_3x3_1_bias',\n",
       " 'conv_4b_double_3x3_1_output',\n",
       " 'bn_4b_double_3x3_1_gamma',\n",
       " 'bn_4b_double_3x3_1_beta',\n",
       " 'bn_4b_double_3x3_1_output',\n",
       " 'relu_4b_double_3x3_1_output',\n",
       " 'avg_pool_4b_pool_output',\n",
       " 'conv_4b_proj_weight',\n",
       " 'conv_4b_proj_bias',\n",
       " 'conv_4b_proj_output',\n",
       " 'bn_4b_proj_gamma',\n",
       " 'bn_4b_proj_beta',\n",
       " 'bn_4b_proj_output',\n",
       " 'relu_4b_proj_output',\n",
       " 'ch_concat_4b_chconcat_output',\n",
       " 'conv_4c_1x1_weight',\n",
       " 'conv_4c_1x1_bias',\n",
       " 'conv_4c_1x1_output',\n",
       " 'bn_4c_1x1_gamma',\n",
       " 'bn_4c_1x1_beta',\n",
       " 'bn_4c_1x1_output',\n",
       " 'relu_4c_1x1_output',\n",
       " 'conv_4c_3x3_reduce_weight',\n",
       " 'conv_4c_3x3_reduce_bias',\n",
       " 'conv_4c_3x3_reduce_output',\n",
       " 'bn_4c_3x3_reduce_gamma',\n",
       " 'bn_4c_3x3_reduce_beta',\n",
       " 'bn_4c_3x3_reduce_output',\n",
       " 'relu_4c_3x3_reduce_output',\n",
       " 'conv_4c_3x3_weight',\n",
       " 'conv_4c_3x3_bias',\n",
       " 'conv_4c_3x3_output',\n",
       " 'bn_4c_3x3_gamma',\n",
       " 'bn_4c_3x3_beta',\n",
       " 'bn_4c_3x3_output',\n",
       " 'relu_4c_3x3_output',\n",
       " 'conv_4c_double_3x3_reduce_weight',\n",
       " 'conv_4c_double_3x3_reduce_bias',\n",
       " 'conv_4c_double_3x3_reduce_output',\n",
       " 'bn_4c_double_3x3_reduce_gamma',\n",
       " 'bn_4c_double_3x3_reduce_beta',\n",
       " 'bn_4c_double_3x3_reduce_output',\n",
       " 'relu_4c_double_3x3_reduce_output',\n",
       " 'conv_4c_double_3x3_0_weight',\n",
       " 'conv_4c_double_3x3_0_bias',\n",
       " 'conv_4c_double_3x3_0_output',\n",
       " 'bn_4c_double_3x3_0_gamma',\n",
       " 'bn_4c_double_3x3_0_beta',\n",
       " 'bn_4c_double_3x3_0_output',\n",
       " 'relu_4c_double_3x3_0_output',\n",
       " 'conv_4c_double_3x3_1_weight',\n",
       " 'conv_4c_double_3x3_1_bias',\n",
       " 'conv_4c_double_3x3_1_output',\n",
       " 'bn_4c_double_3x3_1_gamma',\n",
       " 'bn_4c_double_3x3_1_beta',\n",
       " 'bn_4c_double_3x3_1_output',\n",
       " 'relu_4c_double_3x3_1_output',\n",
       " 'avg_pool_4c_pool_output',\n",
       " 'conv_4c_proj_weight',\n",
       " 'conv_4c_proj_bias',\n",
       " 'conv_4c_proj_output',\n",
       " 'bn_4c_proj_gamma',\n",
       " 'bn_4c_proj_beta',\n",
       " 'bn_4c_proj_output',\n",
       " 'relu_4c_proj_output',\n",
       " 'ch_concat_4c_chconcat_output',\n",
       " 'conv_4d_1x1_weight',\n",
       " 'conv_4d_1x1_bias',\n",
       " 'conv_4d_1x1_output',\n",
       " 'bn_4d_1x1_gamma',\n",
       " 'bn_4d_1x1_beta',\n",
       " 'bn_4d_1x1_output',\n",
       " 'relu_4d_1x1_output',\n",
       " 'conv_4d_3x3_reduce_weight',\n",
       " 'conv_4d_3x3_reduce_bias',\n",
       " 'conv_4d_3x3_reduce_output',\n",
       " 'bn_4d_3x3_reduce_gamma',\n",
       " 'bn_4d_3x3_reduce_beta',\n",
       " 'bn_4d_3x3_reduce_output',\n",
       " 'relu_4d_3x3_reduce_output',\n",
       " 'conv_4d_3x3_weight',\n",
       " 'conv_4d_3x3_bias',\n",
       " 'conv_4d_3x3_output',\n",
       " 'bn_4d_3x3_gamma',\n",
       " 'bn_4d_3x3_beta',\n",
       " 'bn_4d_3x3_output',\n",
       " 'relu_4d_3x3_output',\n",
       " 'conv_4d_double_3x3_reduce_weight',\n",
       " 'conv_4d_double_3x3_reduce_bias',\n",
       " 'conv_4d_double_3x3_reduce_output',\n",
       " 'bn_4d_double_3x3_reduce_gamma',\n",
       " 'bn_4d_double_3x3_reduce_beta',\n",
       " 'bn_4d_double_3x3_reduce_output',\n",
       " 'relu_4d_double_3x3_reduce_output',\n",
       " 'conv_4d_double_3x3_0_weight',\n",
       " 'conv_4d_double_3x3_0_bias',\n",
       " 'conv_4d_double_3x3_0_output',\n",
       " 'bn_4d_double_3x3_0_gamma',\n",
       " 'bn_4d_double_3x3_0_beta',\n",
       " 'bn_4d_double_3x3_0_output',\n",
       " 'relu_4d_double_3x3_0_output',\n",
       " 'conv_4d_double_3x3_1_weight',\n",
       " 'conv_4d_double_3x3_1_bias',\n",
       " 'conv_4d_double_3x3_1_output',\n",
       " 'bn_4d_double_3x3_1_gamma',\n",
       " 'bn_4d_double_3x3_1_beta',\n",
       " 'bn_4d_double_3x3_1_output',\n",
       " 'relu_4d_double_3x3_1_output',\n",
       " 'avg_pool_4d_pool_output',\n",
       " 'conv_4d_proj_weight',\n",
       " 'conv_4d_proj_bias',\n",
       " 'conv_4d_proj_output',\n",
       " 'bn_4d_proj_gamma',\n",
       " 'bn_4d_proj_beta',\n",
       " 'bn_4d_proj_output',\n",
       " 'relu_4d_proj_output',\n",
       " 'ch_concat_4d_chconcat_output',\n",
       " 'conv_4e_3x3_reduce_weight',\n",
       " 'conv_4e_3x3_reduce_bias',\n",
       " 'conv_4e_3x3_reduce_output',\n",
       " 'bn_4e_3x3_reduce_gamma',\n",
       " 'bn_4e_3x3_reduce_beta',\n",
       " 'bn_4e_3x3_reduce_output',\n",
       " 'relu_4e_3x3_reduce_output',\n",
       " 'conv_4e_3x3_weight',\n",
       " 'conv_4e_3x3_bias',\n",
       " 'conv_4e_3x3_output',\n",
       " 'bn_4e_3x3_gamma',\n",
       " 'bn_4e_3x3_beta',\n",
       " 'bn_4e_3x3_output',\n",
       " 'relu_4e_3x3_output',\n",
       " 'conv_4e_double_3x3_reduce_weight',\n",
       " 'conv_4e_double_3x3_reduce_bias',\n",
       " 'conv_4e_double_3x3_reduce_output',\n",
       " 'bn_4e_double_3x3_reduce_gamma',\n",
       " 'bn_4e_double_3x3_reduce_beta',\n",
       " 'bn_4e_double_3x3_reduce_output',\n",
       " 'relu_4e_double_3x3_reduce_output',\n",
       " 'conv_4e_double_3x3_0_weight',\n",
       " 'conv_4e_double_3x3_0_bias',\n",
       " 'conv_4e_double_3x3_0_output',\n",
       " 'bn_4e_double_3x3_0_gamma',\n",
       " 'bn_4e_double_3x3_0_beta',\n",
       " 'bn_4e_double_3x3_0_output',\n",
       " 'relu_4e_double_3x3_0_output',\n",
       " 'conv_4e_double_3x3_1_weight',\n",
       " 'conv_4e_double_3x3_1_bias',\n",
       " 'conv_4e_double_3x3_1_output',\n",
       " 'bn_4e_double_3x3_1_gamma',\n",
       " 'bn_4e_double_3x3_1_beta',\n",
       " 'bn_4e_double_3x3_1_output',\n",
       " 'relu_4e_double_3x3_1_output',\n",
       " 'max_pool_4e_pool_output',\n",
       " 'ch_concat_4e_chconcat_output',\n",
       " 'conv_5a_1x1_weight',\n",
       " 'conv_5a_1x1_bias',\n",
       " 'conv_5a_1x1_output',\n",
       " 'bn_5a_1x1_gamma',\n",
       " 'bn_5a_1x1_beta',\n",
       " 'bn_5a_1x1_output',\n",
       " 'relu_5a_1x1_output',\n",
       " 'conv_5a_3x3_reduce_weight',\n",
       " 'conv_5a_3x3_reduce_bias',\n",
       " 'conv_5a_3x3_reduce_output',\n",
       " 'bn_5a_3x3_reduce_gamma',\n",
       " 'bn_5a_3x3_reduce_beta',\n",
       " 'bn_5a_3x3_reduce_output',\n",
       " 'relu_5a_3x3_reduce_output',\n",
       " 'conv_5a_3x3_weight',\n",
       " 'conv_5a_3x3_bias',\n",
       " 'conv_5a_3x3_output',\n",
       " 'bn_5a_3x3_gamma',\n",
       " 'bn_5a_3x3_beta',\n",
       " 'bn_5a_3x3_output',\n",
       " 'relu_5a_3x3_output',\n",
       " 'conv_5a_double_3x3_reduce_weight',\n",
       " 'conv_5a_double_3x3_reduce_bias',\n",
       " 'conv_5a_double_3x3_reduce_output',\n",
       " 'bn_5a_double_3x3_reduce_gamma',\n",
       " 'bn_5a_double_3x3_reduce_beta',\n",
       " 'bn_5a_double_3x3_reduce_output',\n",
       " 'relu_5a_double_3x3_reduce_output',\n",
       " 'conv_5a_double_3x3_0_weight',\n",
       " 'conv_5a_double_3x3_0_bias',\n",
       " 'conv_5a_double_3x3_0_output',\n",
       " 'bn_5a_double_3x3_0_gamma',\n",
       " 'bn_5a_double_3x3_0_beta',\n",
       " 'bn_5a_double_3x3_0_output',\n",
       " 'relu_5a_double_3x3_0_output',\n",
       " 'conv_5a_double_3x3_1_weight',\n",
       " 'conv_5a_double_3x3_1_bias',\n",
       " 'conv_5a_double_3x3_1_output',\n",
       " 'bn_5a_double_3x3_1_gamma',\n",
       " 'bn_5a_double_3x3_1_beta',\n",
       " 'bn_5a_double_3x3_1_output',\n",
       " 'relu_5a_double_3x3_1_output',\n",
       " 'avg_pool_5a_pool_output',\n",
       " 'conv_5a_proj_weight',\n",
       " 'conv_5a_proj_bias',\n",
       " 'conv_5a_proj_output',\n",
       " 'bn_5a_proj_gamma',\n",
       " 'bn_5a_proj_beta',\n",
       " 'bn_5a_proj_output',\n",
       " 'relu_5a_proj_output',\n",
       " 'ch_concat_5a_chconcat_output',\n",
       " 'conv_5b_1x1_weight',\n",
       " 'conv_5b_1x1_bias',\n",
       " 'conv_5b_1x1_output',\n",
       " 'bn_5b_1x1_gamma',\n",
       " 'bn_5b_1x1_beta',\n",
       " 'bn_5b_1x1_output',\n",
       " 'relu_5b_1x1_output',\n",
       " 'conv_5b_3x3_reduce_weight',\n",
       " 'conv_5b_3x3_reduce_bias',\n",
       " 'conv_5b_3x3_reduce_output',\n",
       " 'bn_5b_3x3_reduce_gamma',\n",
       " 'bn_5b_3x3_reduce_beta',\n",
       " 'bn_5b_3x3_reduce_output',\n",
       " 'relu_5b_3x3_reduce_output',\n",
       " 'conv_5b_3x3_weight',\n",
       " 'conv_5b_3x3_bias',\n",
       " 'conv_5b_3x3_output',\n",
       " 'bn_5b_3x3_gamma',\n",
       " 'bn_5b_3x3_beta',\n",
       " 'bn_5b_3x3_output',\n",
       " 'relu_5b_3x3_output',\n",
       " 'conv_5b_double_3x3_reduce_weight',\n",
       " 'conv_5b_double_3x3_reduce_bias',\n",
       " 'conv_5b_double_3x3_reduce_output',\n",
       " 'bn_5b_double_3x3_reduce_gamma',\n",
       " 'bn_5b_double_3x3_reduce_beta',\n",
       " 'bn_5b_double_3x3_reduce_output',\n",
       " 'relu_5b_double_3x3_reduce_output',\n",
       " 'conv_5b_double_3x3_0_weight',\n",
       " 'conv_5b_double_3x3_0_bias',\n",
       " 'conv_5b_double_3x3_0_output',\n",
       " 'bn_5b_double_3x3_0_gamma',\n",
       " 'bn_5b_double_3x3_0_beta',\n",
       " 'bn_5b_double_3x3_0_output',\n",
       " 'relu_5b_double_3x3_0_output',\n",
       " 'conv_5b_double_3x3_1_weight',\n",
       " 'conv_5b_double_3x3_1_bias',\n",
       " 'conv_5b_double_3x3_1_output',\n",
       " 'bn_5b_double_3x3_1_gamma',\n",
       " 'bn_5b_double_3x3_1_beta',\n",
       " 'bn_5b_double_3x3_1_output',\n",
       " 'relu_5b_double_3x3_1_output',\n",
       " 'max_pool_5b_pool_output',\n",
       " 'conv_5b_proj_weight',\n",
       " 'conv_5b_proj_bias',\n",
       " 'conv_5b_proj_output',\n",
       " 'bn_5b_proj_gamma',\n",
       " 'bn_5b_proj_beta',\n",
       " 'bn_5b_proj_output',\n",
       " 'relu_5b_proj_output',\n",
       " 'ch_concat_5b_chconcat_output',\n",
       " 'global_pool_output',\n",
       " 'flatten_output',\n",
       " 'fc1_weight',\n",
       " 'fc1_bias',\n",
       " 'fc1_output',\n",
       " 'softmax_label',\n",
       " 'softmax_output']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internels.list_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fea_symbol = internels['conv_4d_3x3_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = mx.model.FeedForward(symbol = fea_symbol,numpy_batch_size=1,arg_params=model.arg_params,aux_params=model.aux_params,allow_extra_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor.save('inception_fe_34d',126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inception_fe_34d = mx.model.FeedForward.load('inception_fe_34d', 126, num_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmodel_list = inception_fe_34d.symbol.get_internals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'conv_1_weight',\n",
       " 'conv_1_bias',\n",
       " 'conv_1_output',\n",
       " 'bn_1_gamma',\n",
       " 'bn_1_beta',\n",
       " 'bn_1_output',\n",
       " 'relu_1_output',\n",
       " 'pool_1_output',\n",
       " 'conv_2_red_weight',\n",
       " 'conv_2_red_bias',\n",
       " 'conv_2_red_output',\n",
       " 'bn_2_red_gamma',\n",
       " 'bn_2_red_beta',\n",
       " 'bn_2_red_output',\n",
       " 'relu_2_red_output',\n",
       " 'conv_2_weight',\n",
       " 'conv_2_bias',\n",
       " 'conv_2_output',\n",
       " 'bn_2_gamma',\n",
       " 'bn_2_beta',\n",
       " 'bn_2_output',\n",
       " 'relu_2_output',\n",
       " 'pool_2_output',\n",
       " 'conv_3a_1x1_weight',\n",
       " 'conv_3a_1x1_bias',\n",
       " 'conv_3a_1x1_output',\n",
       " 'bn_3a_1x1_gamma',\n",
       " 'bn_3a_1x1_beta',\n",
       " 'bn_3a_1x1_output',\n",
       " 'relu_3a_1x1_output',\n",
       " 'conv_3a_3x3_reduce_weight',\n",
       " 'conv_3a_3x3_reduce_bias',\n",
       " 'conv_3a_3x3_reduce_output',\n",
       " 'bn_3a_3x3_reduce_gamma',\n",
       " 'bn_3a_3x3_reduce_beta',\n",
       " 'bn_3a_3x3_reduce_output',\n",
       " 'relu_3a_3x3_reduce_output',\n",
       " 'conv_3a_3x3_weight',\n",
       " 'conv_3a_3x3_bias',\n",
       " 'conv_3a_3x3_output',\n",
       " 'bn_3a_3x3_gamma',\n",
       " 'bn_3a_3x3_beta',\n",
       " 'bn_3a_3x3_output',\n",
       " 'relu_3a_3x3_output',\n",
       " 'conv_3a_double_3x3_reduce_weight',\n",
       " 'conv_3a_double_3x3_reduce_bias',\n",
       " 'conv_3a_double_3x3_reduce_output',\n",
       " 'bn_3a_double_3x3_reduce_gamma',\n",
       " 'bn_3a_double_3x3_reduce_beta',\n",
       " 'bn_3a_double_3x3_reduce_output',\n",
       " 'relu_3a_double_3x3_reduce_output',\n",
       " 'conv_3a_double_3x3_0_weight',\n",
       " 'conv_3a_double_3x3_0_bias',\n",
       " 'conv_3a_double_3x3_0_output',\n",
       " 'bn_3a_double_3x3_0_gamma',\n",
       " 'bn_3a_double_3x3_0_beta',\n",
       " 'bn_3a_double_3x3_0_output',\n",
       " 'relu_3a_double_3x3_0_output',\n",
       " 'conv_3a_double_3x3_1_weight',\n",
       " 'conv_3a_double_3x3_1_bias',\n",
       " 'conv_3a_double_3x3_1_output',\n",
       " 'bn_3a_double_3x3_1_gamma',\n",
       " 'bn_3a_double_3x3_1_beta',\n",
       " 'bn_3a_double_3x3_1_output',\n",
       " 'relu_3a_double_3x3_1_output',\n",
       " 'avg_pool_3a_pool_output',\n",
       " 'conv_3a_proj_weight',\n",
       " 'conv_3a_proj_bias',\n",
       " 'conv_3a_proj_output',\n",
       " 'bn_3a_proj_gamma',\n",
       " 'bn_3a_proj_beta',\n",
       " 'bn_3a_proj_output',\n",
       " 'relu_3a_proj_output',\n",
       " 'ch_concat_3a_chconcat_output',\n",
       " 'conv_3b_1x1_weight',\n",
       " 'conv_3b_1x1_bias',\n",
       " 'conv_3b_1x1_output',\n",
       " 'bn_3b_1x1_gamma',\n",
       " 'bn_3b_1x1_beta',\n",
       " 'bn_3b_1x1_output',\n",
       " 'relu_3b_1x1_output',\n",
       " 'conv_3b_3x3_reduce_weight',\n",
       " 'conv_3b_3x3_reduce_bias',\n",
       " 'conv_3b_3x3_reduce_output',\n",
       " 'bn_3b_3x3_reduce_gamma',\n",
       " 'bn_3b_3x3_reduce_beta',\n",
       " 'bn_3b_3x3_reduce_output',\n",
       " 'relu_3b_3x3_reduce_output',\n",
       " 'conv_3b_3x3_weight',\n",
       " 'conv_3b_3x3_bias',\n",
       " 'conv_3b_3x3_output',\n",
       " 'bn_3b_3x3_gamma',\n",
       " 'bn_3b_3x3_beta',\n",
       " 'bn_3b_3x3_output',\n",
       " 'relu_3b_3x3_output',\n",
       " 'conv_3b_double_3x3_reduce_weight',\n",
       " 'conv_3b_double_3x3_reduce_bias',\n",
       " 'conv_3b_double_3x3_reduce_output',\n",
       " 'bn_3b_double_3x3_reduce_gamma',\n",
       " 'bn_3b_double_3x3_reduce_beta',\n",
       " 'bn_3b_double_3x3_reduce_output',\n",
       " 'relu_3b_double_3x3_reduce_output',\n",
       " 'conv_3b_double_3x3_0_weight',\n",
       " 'conv_3b_double_3x3_0_bias',\n",
       " 'conv_3b_double_3x3_0_output',\n",
       " 'bn_3b_double_3x3_0_gamma',\n",
       " 'bn_3b_double_3x3_0_beta',\n",
       " 'bn_3b_double_3x3_0_output',\n",
       " 'relu_3b_double_3x3_0_output',\n",
       " 'conv_3b_double_3x3_1_weight',\n",
       " 'conv_3b_double_3x3_1_bias',\n",
       " 'conv_3b_double_3x3_1_output',\n",
       " 'bn_3b_double_3x3_1_gamma',\n",
       " 'bn_3b_double_3x3_1_beta',\n",
       " 'bn_3b_double_3x3_1_output',\n",
       " 'relu_3b_double_3x3_1_output',\n",
       " 'avg_pool_3b_pool_output',\n",
       " 'conv_3b_proj_weight',\n",
       " 'conv_3b_proj_bias',\n",
       " 'conv_3b_proj_output',\n",
       " 'bn_3b_proj_gamma',\n",
       " 'bn_3b_proj_beta',\n",
       " 'bn_3b_proj_output',\n",
       " 'relu_3b_proj_output',\n",
       " 'ch_concat_3b_chconcat_output',\n",
       " 'conv_3c_3x3_reduce_weight',\n",
       " 'conv_3c_3x3_reduce_bias',\n",
       " 'conv_3c_3x3_reduce_output',\n",
       " 'bn_3c_3x3_reduce_gamma',\n",
       " 'bn_3c_3x3_reduce_beta',\n",
       " 'bn_3c_3x3_reduce_output',\n",
       " 'relu_3c_3x3_reduce_output',\n",
       " 'conv_3c_3x3_weight',\n",
       " 'conv_3c_3x3_bias',\n",
       " 'conv_3c_3x3_output',\n",
       " 'bn_3c_3x3_gamma',\n",
       " 'bn_3c_3x3_beta',\n",
       " 'bn_3c_3x3_output',\n",
       " 'relu_3c_3x3_output',\n",
       " 'conv_3c_double_3x3_reduce_weight',\n",
       " 'conv_3c_double_3x3_reduce_bias',\n",
       " 'conv_3c_double_3x3_reduce_output',\n",
       " 'bn_3c_double_3x3_reduce_gamma',\n",
       " 'bn_3c_double_3x3_reduce_beta',\n",
       " 'bn_3c_double_3x3_reduce_output',\n",
       " 'relu_3c_double_3x3_reduce_output',\n",
       " 'conv_3c_double_3x3_0_weight',\n",
       " 'conv_3c_double_3x3_0_bias',\n",
       " 'conv_3c_double_3x3_0_output',\n",
       " 'bn_3c_double_3x3_0_gamma',\n",
       " 'bn_3c_double_3x3_0_beta',\n",
       " 'bn_3c_double_3x3_0_output',\n",
       " 'relu_3c_double_3x3_0_output',\n",
       " 'conv_3c_double_3x3_1_weight',\n",
       " 'conv_3c_double_3x3_1_bias',\n",
       " 'conv_3c_double_3x3_1_output',\n",
       " 'bn_3c_double_3x3_1_gamma',\n",
       " 'bn_3c_double_3x3_1_beta',\n",
       " 'bn_3c_double_3x3_1_output',\n",
       " 'relu_3c_double_3x3_1_output',\n",
       " 'max_pool_3c_pool_output',\n",
       " 'ch_concat_3c_chconcat_output',\n",
       " 'conv_4a_1x1_weight',\n",
       " 'conv_4a_1x1_bias',\n",
       " 'conv_4a_1x1_output',\n",
       " 'bn_4a_1x1_gamma',\n",
       " 'bn_4a_1x1_beta',\n",
       " 'bn_4a_1x1_output',\n",
       " 'relu_4a_1x1_output',\n",
       " 'conv_4a_3x3_reduce_weight',\n",
       " 'conv_4a_3x3_reduce_bias',\n",
       " 'conv_4a_3x3_reduce_output',\n",
       " 'bn_4a_3x3_reduce_gamma',\n",
       " 'bn_4a_3x3_reduce_beta',\n",
       " 'bn_4a_3x3_reduce_output',\n",
       " 'relu_4a_3x3_reduce_output',\n",
       " 'conv_4a_3x3_weight',\n",
       " 'conv_4a_3x3_bias',\n",
       " 'conv_4a_3x3_output',\n",
       " 'bn_4a_3x3_gamma',\n",
       " 'bn_4a_3x3_beta',\n",
       " 'bn_4a_3x3_output',\n",
       " 'relu_4a_3x3_output',\n",
       " 'conv_4a_double_3x3_reduce_weight',\n",
       " 'conv_4a_double_3x3_reduce_bias',\n",
       " 'conv_4a_double_3x3_reduce_output',\n",
       " 'bn_4a_double_3x3_reduce_gamma',\n",
       " 'bn_4a_double_3x3_reduce_beta',\n",
       " 'bn_4a_double_3x3_reduce_output',\n",
       " 'relu_4a_double_3x3_reduce_output',\n",
       " 'conv_4a_double_3x3_0_weight',\n",
       " 'conv_4a_double_3x3_0_bias',\n",
       " 'conv_4a_double_3x3_0_output',\n",
       " 'bn_4a_double_3x3_0_gamma',\n",
       " 'bn_4a_double_3x3_0_beta',\n",
       " 'bn_4a_double_3x3_0_output',\n",
       " 'relu_4a_double_3x3_0_output',\n",
       " 'conv_4a_double_3x3_1_weight',\n",
       " 'conv_4a_double_3x3_1_bias',\n",
       " 'conv_4a_double_3x3_1_output',\n",
       " 'bn_4a_double_3x3_1_gamma',\n",
       " 'bn_4a_double_3x3_1_beta',\n",
       " 'bn_4a_double_3x3_1_output',\n",
       " 'relu_4a_double_3x3_1_output',\n",
       " 'avg_pool_4a_pool_output',\n",
       " 'conv_4a_proj_weight',\n",
       " 'conv_4a_proj_bias',\n",
       " 'conv_4a_proj_output',\n",
       " 'bn_4a_proj_gamma',\n",
       " 'bn_4a_proj_beta',\n",
       " 'bn_4a_proj_output',\n",
       " 'relu_4a_proj_output',\n",
       " 'ch_concat_4a_chconcat_output',\n",
       " 'conv_4b_1x1_weight',\n",
       " 'conv_4b_1x1_bias',\n",
       " 'conv_4b_1x1_output',\n",
       " 'bn_4b_1x1_gamma',\n",
       " 'bn_4b_1x1_beta',\n",
       " 'bn_4b_1x1_output',\n",
       " 'relu_4b_1x1_output',\n",
       " 'conv_4b_3x3_reduce_weight',\n",
       " 'conv_4b_3x3_reduce_bias',\n",
       " 'conv_4b_3x3_reduce_output',\n",
       " 'bn_4b_3x3_reduce_gamma',\n",
       " 'bn_4b_3x3_reduce_beta',\n",
       " 'bn_4b_3x3_reduce_output',\n",
       " 'relu_4b_3x3_reduce_output',\n",
       " 'conv_4b_3x3_weight',\n",
       " 'conv_4b_3x3_bias',\n",
       " 'conv_4b_3x3_output',\n",
       " 'bn_4b_3x3_gamma',\n",
       " 'bn_4b_3x3_beta',\n",
       " 'bn_4b_3x3_output',\n",
       " 'relu_4b_3x3_output',\n",
       " 'conv_4b_double_3x3_reduce_weight',\n",
       " 'conv_4b_double_3x3_reduce_bias',\n",
       " 'conv_4b_double_3x3_reduce_output',\n",
       " 'bn_4b_double_3x3_reduce_gamma',\n",
       " 'bn_4b_double_3x3_reduce_beta',\n",
       " 'bn_4b_double_3x3_reduce_output',\n",
       " 'relu_4b_double_3x3_reduce_output',\n",
       " 'conv_4b_double_3x3_0_weight',\n",
       " 'conv_4b_double_3x3_0_bias',\n",
       " 'conv_4b_double_3x3_0_output',\n",
       " 'bn_4b_double_3x3_0_gamma',\n",
       " 'bn_4b_double_3x3_0_beta',\n",
       " 'bn_4b_double_3x3_0_output',\n",
       " 'relu_4b_double_3x3_0_output',\n",
       " 'conv_4b_double_3x3_1_weight',\n",
       " 'conv_4b_double_3x3_1_bias',\n",
       " 'conv_4b_double_3x3_1_output',\n",
       " 'bn_4b_double_3x3_1_gamma',\n",
       " 'bn_4b_double_3x3_1_beta',\n",
       " 'bn_4b_double_3x3_1_output',\n",
       " 'relu_4b_double_3x3_1_output',\n",
       " 'avg_pool_4b_pool_output',\n",
       " 'conv_4b_proj_weight',\n",
       " 'conv_4b_proj_bias',\n",
       " 'conv_4b_proj_output',\n",
       " 'bn_4b_proj_gamma',\n",
       " 'bn_4b_proj_beta',\n",
       " 'bn_4b_proj_output',\n",
       " 'relu_4b_proj_output',\n",
       " 'ch_concat_4b_chconcat_output',\n",
       " 'conv_4c_1x1_weight',\n",
       " 'conv_4c_1x1_bias',\n",
       " 'conv_4c_1x1_output',\n",
       " 'bn_4c_1x1_gamma',\n",
       " 'bn_4c_1x1_beta',\n",
       " 'bn_4c_1x1_output',\n",
       " 'relu_4c_1x1_output',\n",
       " 'conv_4c_3x3_reduce_weight',\n",
       " 'conv_4c_3x3_reduce_bias',\n",
       " 'conv_4c_3x3_reduce_output',\n",
       " 'bn_4c_3x3_reduce_gamma',\n",
       " 'bn_4c_3x3_reduce_beta',\n",
       " 'bn_4c_3x3_reduce_output',\n",
       " 'relu_4c_3x3_reduce_output',\n",
       " 'conv_4c_3x3_weight',\n",
       " 'conv_4c_3x3_bias',\n",
       " 'conv_4c_3x3_output',\n",
       " 'bn_4c_3x3_gamma',\n",
       " 'bn_4c_3x3_beta',\n",
       " 'bn_4c_3x3_output',\n",
       " 'relu_4c_3x3_output',\n",
       " 'conv_4c_double_3x3_reduce_weight',\n",
       " 'conv_4c_double_3x3_reduce_bias',\n",
       " 'conv_4c_double_3x3_reduce_output',\n",
       " 'bn_4c_double_3x3_reduce_gamma',\n",
       " 'bn_4c_double_3x3_reduce_beta',\n",
       " 'bn_4c_double_3x3_reduce_output',\n",
       " 'relu_4c_double_3x3_reduce_output',\n",
       " 'conv_4c_double_3x3_0_weight',\n",
       " 'conv_4c_double_3x3_0_bias',\n",
       " 'conv_4c_double_3x3_0_output',\n",
       " 'bn_4c_double_3x3_0_gamma',\n",
       " 'bn_4c_double_3x3_0_beta',\n",
       " 'bn_4c_double_3x3_0_output',\n",
       " 'relu_4c_double_3x3_0_output',\n",
       " 'conv_4c_double_3x3_1_weight',\n",
       " 'conv_4c_double_3x3_1_bias',\n",
       " 'conv_4c_double_3x3_1_output',\n",
       " 'bn_4c_double_3x3_1_gamma',\n",
       " 'bn_4c_double_3x3_1_beta',\n",
       " 'bn_4c_double_3x3_1_output',\n",
       " 'relu_4c_double_3x3_1_output',\n",
       " 'avg_pool_4c_pool_output',\n",
       " 'conv_4c_proj_weight',\n",
       " 'conv_4c_proj_bias',\n",
       " 'conv_4c_proj_output',\n",
       " 'bn_4c_proj_gamma',\n",
       " 'bn_4c_proj_beta',\n",
       " 'bn_4c_proj_output',\n",
       " 'relu_4c_proj_output',\n",
       " 'ch_concat_4c_chconcat_output',\n",
       " 'conv_4d_3x3_reduce_weight',\n",
       " 'conv_4d_3x3_reduce_bias',\n",
       " 'conv_4d_3x3_reduce_output',\n",
       " 'bn_4d_3x3_reduce_gamma',\n",
       " 'bn_4d_3x3_reduce_beta',\n",
       " 'bn_4d_3x3_reduce_output',\n",
       " 'relu_4d_3x3_reduce_output',\n",
       " 'conv_4d_3x3_weight',\n",
       " 'conv_4d_3x3_bias',\n",
       " 'conv_4d_3x3_output']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel_list.list_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Module in module mxnet.module.module:\n",
      "\n",
      "class Module(mxnet.module.base_module.BaseModule)\n",
      " |  Module is a basic module that wrap a `Symbol`. It is functionally the same\n",
      " |  as the `FeedForward` model, except under the module API.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  symbol : Symbol\n",
      " |  data_names : list of str\n",
      " |      Default is `('data')` for a typical model used in image classification.\n",
      " |  label_names : list of str\n",
      " |      Default is `('softmax_label')` for a typical model used in image\n",
      " |      classification.\n",
      " |  logger : Logger\n",
      " |      Default is `logging`.\n",
      " |  context : Context or list of Context\n",
      " |      Default is `cpu()`.\n",
      " |  work_load_list : list of number\n",
      " |      Default `None`, indicating uniform workload.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Module\n",
      " |      mxnet.module.base_module.BaseModule\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, symbol, data_names=('data',), label_names=('softmax_label',), logger=<module 'logging' from 'c:\\Anaconda2\\lib\\logging\\__init__.pyc'>, context=cpu(0), work_load_list=None)\n",
      " |  \n",
      " |  backward(self, out_grads=None)\n",
      " |      Backward computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      out_grads : NDArray or list of NDArray, optional\n",
      " |          Gradient on the outputs to be propagated back.\n",
      " |          This parameter is only needed when bind is called\n",
      " |          on outputs that are not a loss function.\n",
      " |  \n",
      " |  bind(self, data_shapes, label_shapes=None, for_training=True, inputs_need_grad=False, force_rebind=False, shared_module=None)\n",
      " |      Bind the symbols to construct executors. This is necessary before one\n",
      " |      can perform computation with the module.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data_shapes : list of (str, tuple)\n",
      " |          Typically is `data_iter.provide_data`.\n",
      " |      label_shapes : list of (str, tuple)\n",
      " |          Typically is `data_iter.provide_label`.\n",
      " |      for_training : bool\n",
      " |          Default is `True`. Whether the executors should be bind for training.\n",
      " |      inputs_need_grad : bool\n",
      " |          Default is `False`. Whether the gradients to the input data need to be computed.\n",
      " |          Typically this is not needed. But this might be needed when implementing composition\n",
      " |          of modules.\n",
      " |      force_rebind : bool\n",
      " |          Default is `False`. This function does nothing if the executors are already\n",
      " |          binded. But with this `True`, the executors will be forced to rebind.\n",
      " |      shared_module : Module\n",
      " |          Default is `None`. This is used in bucketing. When not `None`, the shared module\n",
      " |          essentially corresponds to a different bucket -- a module with different symbol\n",
      " |          but with the same sets of parameters (e.g. unrolled RNNs with different lengths).\n",
      " |  \n",
      " |  borrow_optimizer(self, shared_module)\n",
      " |      Borrow optimizer from a shared module. Used in bucketing, where exactly the same\n",
      " |      optimizer (esp. kvstore) is used.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shared_module : Module\n",
      " |  \n",
      " |  forward(self, data_batch, is_train=None)\n",
      " |      Forward computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data_batch : DataBatch\n",
      " |          Could be anything with similar API implemented.\n",
      " |      is_train : bool\n",
      " |          Default is `None`, which means `is_train` takes the value of `self.for_training`.\n",
      " |  \n",
      " |  get_input_grads(self, merge_multi_context=True)\n",
      " |      Get the gradients with respect to the inputs of the module.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      merge_multi_context : bool\n",
      " |          Default is `True`. In the case when data-parallelism is used, the outputs\n",
      " |          will be collected from multiple devices. A `True` value indicate that we\n",
      " |          should merge the collected results so that they look like from a single\n",
      " |          executor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If `merge_multi_context` is `True`, it is like `[grad1, grad2]`. Otherwise, it\n",
      " |      is like `[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]`. All the output\n",
      " |      elements are `NDArray`.\n",
      " |  \n",
      " |  get_outputs(self, merge_multi_context=True)\n",
      " |      Get outputs of the previous forward computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      merge_multi_context : bool\n",
      " |          Default is `True`. In the case when data-parallelism is used, the outputs\n",
      " |          will be collected from multiple devices. A `True` value indicate that we\n",
      " |          should merge the collected results so that they look like from a single\n",
      " |          executor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If `merge_multi_context` is `True`, it is like `[out1, out2]`. Otherwise, it\n",
      " |      is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output\n",
      " |      elements are `NDArray`.\n",
      " |  \n",
      " |  get_params(self)\n",
      " |      Get current parameters.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      `(arg_params, aux_params)`, each a dictionary of name to parameters (in\n",
      " |      `NDArray`) mapping.\n",
      " |  \n",
      " |  init_optimizer(self, kvstore='local', optimizer='sgd', optimizer_params=(('learning_rate', 0.01),), force_init=False)\n",
      " |      Install and initialize optimizers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      kvstore : str or KVStore\n",
      " |          Default `'local'`.\n",
      " |      optimizer : str or Optimizer\n",
      " |          Default `'sgd'`\n",
      " |      optimizer_params : dict\n",
      " |          Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,\n",
      " |          just to avoid pylint warning of dangerous default values.\n",
      " |      force_init : bool\n",
      " |          Default `False`, indicating whether we should force re-initializing the\n",
      " |          optimizer in the case an optimizer is already installed.\n",
      " |  \n",
      " |  init_params(self, initializer=<mxnet.initializer.Uniform object>, arg_params=None, aux_params=None, allow_missing=False, force_init=False)\n",
      " |      Initialize the parameters and auxiliary states.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      initializer : Initializer\n",
      " |          Called to initialize parameters if needed.\n",
      " |      arg_params : dict\n",
      " |          If not None, should be a dictionary of existing arg_params. Initialization\n",
      " |          will be copied from that.\n",
      " |      aux_params : dict\n",
      " |          If not None, should be a dictionary of existing aux_params. Initialization\n",
      " |          will be copied from that.\n",
      " |      allow_missing : bool\n",
      " |          If true, params could contain missing values, and the initializer will be\n",
      " |          called to fill those missing params.\n",
      " |      force_init : bool\n",
      " |          If true, will force re-initialize even if already initialized.\n",
      " |  \n",
      " |  update(self)\n",
      " |      Update parameters according to the installed optimizer and the gradients computed\n",
      " |      in the previous forward-backward batch.\n",
      " |  \n",
      " |  update_metric(self, eval_metric, labels)\n",
      " |      Evaluate and accumulate evaluation metric on outputs of the last forward computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eval_metric : EvalMetric\n",
      " |      labels : list of NDArray\n",
      " |          Typically `data_batch.label`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  data_names\n",
      " |      A list of names for data required by this module.\n",
      " |  \n",
      " |  data_shapes\n",
      " |      Get data shapes.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      A list of `(name, shape)` pairs.\n",
      " |  \n",
      " |  label_shapes\n",
      " |      Get label shapes.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      A list of `(name, shape)` pairs. The return value could be `None` if\n",
      " |      the module does not need labels, or if the module is not binded for\n",
      " |      training (in this case, label information is not available).\n",
      " |  \n",
      " |  output_names\n",
      " |      A list of names for the outputs of this module.\n",
      " |  \n",
      " |  output_shapes\n",
      " |      Get output shapes.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      A list of `(name, shape)` pairs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.module.base_module.BaseModule:\n",
      " |  \n",
      " |  fit(self, train_data, eval_data=None, eval_metric='acc', epoch_end_callback=None, batch_end_callback=None, kvstore='local', optimizer='sgd', optimizer_params=(('learning_rate', 0.01),), eval_batch_end_callback=None, initializer=<mxnet.initializer.Uniform object>, arg_params=None, aux_params=None, allow_missing=False, force_rebind=False, force_init=False, begin_epoch=0, num_epoch=None)\n",
      " |      Train the module parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_data : DataIter\n",
      " |      eval_data : DataIter\n",
      " |          If not `None`, will be used as validation set and evaluate the performance\n",
      " |          after each epoch.\n",
      " |      eval_metric : str or EvalMetric\n",
      " |          Default `'acc'`. The performance measure used to display during training.\n",
      " |      epoch_end_callback : function or list of function\n",
      " |          Each callback will be called with the current `epoch`, `symbol`, `arg_params`\n",
      " |          and `aux_params`.\n",
      " |      batch_end_callback : function or list of function\n",
      " |          Each callback will be called with a `BatchEndParam`.\n",
      " |      kvstore : str or KVStore\n",
      " |          Default `'local'`.\n",
      " |      optimizer : str or Optimizer\n",
      " |          Default `'sgd'`\n",
      " |      optimizer_params : dict\n",
      " |          Default `(('learning_rate', 0.01),)`. The parameters for the optimizer constructor.\n",
      " |          The default value is not a `dict`, just to avoid pylint warning on dangerous\n",
      " |          default values.\n",
      " |      eval_batch_end_callback : function or list of function\n",
      " |      initializer : Initializer\n",
      " |          Will be called to initialize the module parameters if not already initialized.\n",
      " |      arg_params : dict\n",
      " |          Default `None`, if not `None`, should be existing parameters from a trained\n",
      " |          model or loaded from a checkpoint (previously saved model). In this case,\n",
      " |          the value here will be used to initialize the module parameters, unless they\n",
      " |          are already initialized by the user via a call to `init_params` or `fit`.\n",
      " |          `arg_params` has higher priority to `initializer`.\n",
      " |      aux_params : dict\n",
      " |          Default `None`. Similar to `arg_params`, except for auxiliary states.\n",
      " |      allow_missing : bool\n",
      " |          Default `False`. Indicate whether we allow missing parameters when `arg_params`\n",
      " |          and `aux_params` are not `None`. If this is `True`, then the missing parameters\n",
      " |          will be initialized via the `initializer`.\n",
      " |      force_rebind : bool\n",
      " |          Default `False`. Whether to force rebinding the executors if already binded.\n",
      " |      force_init : bool\n",
      " |          Default `False`. Indicate whether we should force initialization even if the\n",
      " |          parameters are already initialized.\n",
      " |      begin_epoch : int\n",
      " |          Default `0`. Indicate the starting epoch. Usually, if we are resuming from a\n",
      " |          checkpoint saved at a previous training phase at epoch N, then we should specify\n",
      " |          this value as N+1.\n",
      " |      num_epoch : int\n",
      " |          Number of epochs to run training.\n",
      " |  \n",
      " |  forward_backward(self, data_batch)\n",
      " |      A convenient function that calls both `forward` and `backward`.\n",
      " |  \n",
      " |  iter_predict(self, eval_data, num_batch=None, reset=True)\n",
      " |      Iterate over predictions.\n",
      " |      \n",
      " |          for pred, i_batch, batch in module.iter_predict(eval_data):\n",
      " |              # pred is a list of outputs from the module\n",
      " |              # i_batch is a integer\n",
      " |              # batch is the data batch from the data iterator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eval_data : DataIter\n",
      " |      num_batch : int\n",
      " |          Default is `None`, indicating running all the batches in the data iterator.\n",
      " |      reset : bool\n",
      " |          Default is `True`, indicating whether we should reset the data iter before start\n",
      " |          doing prediction.\n",
      " |  \n",
      " |  load_params(self, fname)\n",
      " |      Load model parameters from file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to input param file.\n",
      " |  \n",
      " |  predict(self, eval_data, num_batch=None, merge_batches=True, reset=True, always_output_list=False)\n",
      " |      Run prediction and collect the outputs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eval_data : DataIter\n",
      " |      num_batch : int\n",
      " |          Default is `None`, indicating running all the batches in the data iterator.\n",
      " |      merge_batches : bool\n",
      " |          Default is `True`, see the doc for return values.\n",
      " |      reset : bool\n",
      " |          Default is `True`, indicating whether we should reset the data iter before start\n",
      " |          doing prediction.\n",
      " |      always_output_list : bool\n",
      " |          Default is `False`, see the doc for return values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      When `merge_batches` is `True` (by default), the return value will be a list\n",
      " |      `[out1, out2, out3]`.  Where each element is concatenation of the outputs for\n",
      " |      all the mini-batches. If further that `always_output_list` is `False` (by default),\n",
      " |      then in the case of a single output, `out1` is returned instead of `[out1]`.\n",
      " |      \n",
      " |      When `merge_batches` is `False`, the return value will be a nested list like\n",
      " |      `[[out1_batch1, out2_batch1], [out1_batch2], ...]`. This mode is useful because\n",
      " |      in some cases (e.g. bucketing), the module does not necessarily produce the same\n",
      " |      number of outputs.\n",
      " |      \n",
      " |      The objects in the results are `NDArray`s. If you need to work with numpy array,\n",
      " |      just call `.asnumpy()` on each of the `NDArray`.\n",
      " |  \n",
      " |  save_params(self, fname)\n",
      " |      Save model parameters to file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to output param file.\n",
      " |  \n",
      " |  score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None, reset=True, epoch=0)\n",
      " |      Run prediction on `eval_data` and evaluate the performance according to\n",
      " |      `eval_metric`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eval_data : DataIter\n",
      " |      eval_metric : EvalMetric\n",
      " |      num_batch : int\n",
      " |          Number of batches to run. Default is `None`, indicating run until the `DataIter`\n",
      " |          finishes.\n",
      " |      batch_end_callback : function\n",
      " |          Could also be a list of functions.\n",
      " |      reset : bool\n",
      " |          Default `True`, indicating whether we should reset `eval_data` before starting\n",
      " |          evaluating.\n",
      " |      epoch : int\n",
      " |          Default 0. For compatibility, this will be passed to callbacks (if any). During\n",
      " |          training, this will correspond to the training epoch number.\n",
      " |  \n",
      " |  set_params(self, arg_params, aux_params)\n",
      " |      Assign parameter and aux state values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg_params : dict\n",
      " |          Dictionary of name to value (`NDArray`) mapping.\n",
      " |      aux_params : dict\n",
      " |          Dictionary of name to value (`NDArray`) mapping.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.module.base_module.BaseModule:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  symbol\n",
      " |      Get the symbol associated with this module.\n",
      " |      \n",
      " |      Except for `Module`, for other types of modules (e.g. `BucketingModule`), this\n",
      " |      property might not be a constant throughout its life time. Some modules might\n",
      " |      not even be associated with any symbols.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "help(mx.mod.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
