+htlnoroomrate_60d  # 11.7423540
#                +noroomrate_60d
,data = SHTtrain
,distribution = "bernoulli"
,n.trees = 1000
,shrinkage = 0.02
,interaction.depth = 3
,bag.fraction = 1
,train.fraction=0.5
,cv.folds=5
#               ,n.minobsinnode = 10
)
# summary(gbm.sht)
best.raw <- gbm.perf(gbm.sht, method='cv')
# Roc curve
test$pred <- predict(gbm.sht, newdata= test, n.trees = best.raw, type="response")
roc[i] <- gbm.roc.area(test$isfullroom, test$pred)  # 0.557468
}
roc
length(threaf)
for(k in 1:length(threaf))
{
trainsample <- rbinom(dim(SHT)[[1]], 1, threaf[k])==1
# split train & test result
SHTtrain <- SHT[trainsample,]
SHTtest <- SHT[!trainsample,]
# responsetrain <- filter(SHTtrain, isfullroom==1)
# responsetest <- filter(SHTtest, isfullroom==1)
# # GBM模型
gbm.sht <- gbm(isfullroom ~
#                 factor(cprflag)
#                +factor(submitfrom)
factor(ordprior)
#                +factor(orddays)
#                +factor(ordroomnum)
#                +factor(level)
+factor(goldstar)
#                +factor(roomtypestd)
#                +factor(basicroomnum)
+factor(recommendlevel)
#                +defaultrecommendlevel
+ordhour
#                +ordamount
#                +last3dnoroom_htl
#                +last7dnoroom_htl
#                +last30dnoroom_htl
#                +last3droom_htl
#                +last7droom_htl
#                +last30droom_htl
+last3dnoroom_sht
#                +last3dnoroom
#                +last7dnoroom_sht
#                +last7dnoroom
#                +last30dnoroom_sht
#                +last30dnoroom
+gintense
+star
+gcity
+gzone
+last3droom
+last3droom_sht
+cityi
+htlnoroomorders_7d   # 37.0360131
+zonestari
+last7droom
+zonei
+last7droom_sht
+intense
#                +last30droom
#                +last30droom_sht
#                +htlnoroomrate_7d
#                +htltotalorders_7d
#                +last3dmedianprice
#                +last3davgprice_sht
#                +last7dmedianprice
#                +last30dmedianprice
#                +last7davgprice_sht
#                +last30avgprice_sht
#                +roomquantity
#                +zonenoroomorders_3d
#                +hotelroomquantity
#                +shtorders_60d
#                +zonetotalorders_3d
#                +htlorders_60d
+shtnoroomrate_60d
+zonenoroomrate_3d
#                +totalorders_60d
+htlnoroomrate_60d  # 11.7423540
#                +noroomrate_60d
,data = SHTtrain
,distribution = "bernoulli"
,n.trees = 1000
,shrinkage = 0.02
,interaction.depth = 3
,bag.fraction = 1
,train.fraction=0.5
,cv.folds=5
#               ,n.minobsinnode = 10
)
# summary(gbm.sht)
best.raw <- gbm.perf(gbm.sht, method='cv')
# Roc curve
test$pred <- predict(gbm.sht, newdata= test, n.trees = best.raw, type="response")
roc[k] <- gbm.roc.area(test$isfullroom, test$pred)  # 0.557468
}
x <- rnorm(2000)
hist(x,freq=F,breaks=30)
lines(density(x,bw=5,col='red'))
library(ggplot2)
p <- ggplot(x,aes(x=x,y=..density..))
p <- ggplot(data.frame(x),aes(x=x,y=..density..))
p+geom_histogram(fill='navy')+geom_density(colour='green')
library(reshape2)
library(reshape2)
hp <- ggplot(tips, aes(x=total_bill)) + geom_histogram(binwidth=2,colour="white")
# Histogram of total_bill, divided by sex and smoker
hp + facet_grid(sex ~ smoker)
# Same as above, with scales="free_y"
hp + facet_grid(sex ~ smoker, scales="free_y")
# With panels that have the same scaling, but different range (and therefore different physical sizes)
hp + facet_grid(sex ~ smoker, scales="free", space="free")
rm(list=ls(all=TRUE))
install.packages("XLConnect")
library(randomForest)
getTree(randomForest(iris[,-5], iris[,5], ntree=10), 3, labelVar=TRUE)
names(iris)
iris[,-5]
iris[,5]
aa <- randomForest(iris[,-5], iris[,5], ntree=10)
partiaPlot(aa)
partialPlot(aa)
partialPlot(aa,x.var=3)
plot(aa)
library(rpart.plot)
library(rattle)
library(rpart)
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
fancyRpartPlot(fit)
install.packages("rpart.plot")
install.packages("rattle")
library(rpart.plot)
library(rattle)
library(rpart)
fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
fancyRpartPlot(fit)
asRules(fit)
install.packages("xgboost")
install.packages("xgboost")
install.packages("xgboost", contriburl="https://github.com/dmlc/xgboost")
install.packages("data.table")
install.packages("~/百度云同步盘/R/packages/xgboost_0.4-2.tgz", repos = NULL)
library(xgboost)
install.packages("devtools")
library(devtools)
install_github(repo="https://github.com/dmlc/xgboost/tree/master/R-package")
install_github("https://github.com/dmlc/xgboost/tree/master/R-package")
install_github("/dmlc/xgboost/tree/master/R-package")
install_github("/R-package")
install.packages("~/百度云同步盘/R/packages/data.table_1.9.4.tgz", repos = NULL)
install.packages("~/Downloads/xgboost_0.4-2.tgz", repos = NULL)
devtools::install_github('dmlc/xgboost',subdir='R-package')
library("xgboost", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
remove.packages("xgboost")
install.packages("~/Downloads/data.table_1.9.4.tgz", repos = NULL)
installed.packages()
head(installed.packages())
devtools::install_github('dmlc/xgboost',subdir='R-package')
install.packages("data.table")
library(xgboost)
install.packages("xgboost")
devtools::install_github(repo="xgboost")
devtools::install_github(xgboost)
devtools::install_github("xgboost")
install.packages("~/Downloads/xgboost_0.4-2.tgz", repos = NULL)
library(xgboost)
#
# Code to accompany "A Simpler Explanation of Differential Privacy"
# [LINK]
# This simulates the game described in the blog post
#
library(ggplot2) # For the graphs at the bottom of the code
set.seed(345345)
rlaplace <- function(n,sigma) {
if(sigma<=0) {
numeric(n)
}
rexp(n,rate = 1/sigma) - rexp(n,rate = 1/sigma)
}
collar = function(x) {
pmax(0, pmin(1, x))
}
# Play N rounds of the differential privacy game
run_exp = function(n, threshold, Nruns, epsilon) {
# The noise parameter.
# Typically, practitioners use k/epsilon count, with k in
# range(1,100). We divide by Nruns because we are working
# in frequencies, not counts
sigma = (20/epsilon)/Nruns
# The Universe is two sets, S and S'
s = numeric(n);
sprime = numeric(n); sprime[1] =1
sets = list(s, sprime)
# this is run inside the "private world"
estimate_s = collar(mean(s) + rlaplace(Nruns, sigma))
estimate_prime = collar(mean(sprime) + rlaplace(Nruns, sigma))
# this is what the adversary sees
outcome_s = estimate_s >= threshold
outcome_prime = estimate_prime >= threshold
ps = mean(outcome_s)
pprime = mean(outcome_prime)
# epsilon-dp means abs(log(ps/pprime)) < epsilon
logratio = abs(log(ps/pprime))
data.frame(mean_s = mean(estimate_s),
mean_prime = mean(estimate_prime),
logratio = logratio)
}
n = 100; threshold=1/200; Nruns = 1000
epsilon  = seq(from=0, to=0.25, by=0.01)
epsilon[1] = 0.001  # can't do zero
runframe = NULL
for(eps in epsilon) {
run = cbind(run_exp(n, threshold, Nruns, eps), epsilon=eps)
runframe = rbind(run, runframe)
}
runframe$diff = with(runframe, abs(mean_s-mean_prime)/pmax(mean_s, mean_prime))
# As epsilon gets stricter (smaller), the relative difference in the
# estimates of the means of S and S' gets smaller, as you would hope.
# The discrepancies in the graph are because sigma
# wasn't set correctly sometimes (especially for the stricter epsilons),
# but the trend is clear.
ggplot(runframe, aes(x=epsilon, y=diff)) +
geom_point() + geom_line() + geom_smooth() +
ggtitle("relative gap in estimates")
# However, as epsilon gets stricter, the estimates also become poorer,
# relative to the actual set means (0 and 0.01, respectively)
estimateframe = melt(runframe, measure.vars=c("mean_s", "mean_prime"), variable.name="set", value.name="mean_value")
ggplot(estimateframe, aes(x=epsilon, y=mean_value, color=set)) + geom_line() +
geom_hline(yintercept = 0.01) + scale_color_manual(values=c("#1b9e77", "#d95f02")) +
ggtitle("actual estimates")
library(reshape2)
#
# Code to accompany "A Simpler Explanation of Differential Privacy"
# [LINK]
# This simulates the game described in the blog post
#
library(ggplot2) # For the graphs at the bottom of the code
library(reshape2)
set.seed(345345)
rlaplace <- function(n,sigma) {
if(sigma<=0) {
numeric(n)
}
rexp(n,rate = 1/sigma) - rexp(n,rate = 1/sigma)
}
collar = function(x) {
pmax(0, pmin(1, x))
}
# Play N rounds of the differential privacy game
run_exp = function(n, threshold, Nruns, epsilon) {
# The noise parameter.
# Typically, practitioners use k/epsilon count, with k in
# range(1,100). We divide by Nruns because we are working
# in frequencies, not counts
sigma = (20/epsilon)/Nruns
# The Universe is two sets, S and S'
s = numeric(n);
sprime = numeric(n); sprime[1] =1
sets = list(s, sprime)
# this is run inside the "private world"
estimate_s = collar(mean(s) + rlaplace(Nruns, sigma))
estimate_prime = collar(mean(sprime) + rlaplace(Nruns, sigma))
# this is what the adversary sees
outcome_s = estimate_s >= threshold
outcome_prime = estimate_prime >= threshold
ps = mean(outcome_s)
pprime = mean(outcome_prime)
# epsilon-dp means abs(log(ps/pprime)) < epsilon
logratio = abs(log(ps/pprime))
data.frame(mean_s = mean(estimate_s),
mean_prime = mean(estimate_prime),
logratio = logratio)
}
n = 100; threshold=1/200; Nruns = 1000
epsilon  = seq(from=0, to=0.25, by=0.01)
epsilon[1] = 0.001  # can't do zero
runframe = NULL
for(eps in epsilon) {
run = cbind(run_exp(n, threshold, Nruns, eps), epsilon=eps)
runframe = rbind(run, runframe)
}
runframe$diff = with(runframe, abs(mean_s-mean_prime)/pmax(mean_s, mean_prime))
# As epsilon gets stricter (smaller), the relative difference in the
# estimates of the means of S and S' gets smaller, as you would hope.
# The discrepancies in the graph are because sigma
# wasn't set correctly sometimes (especially for the stricter epsilons),
# but the trend is clear.
ggplot(runframe, aes(x=epsilon, y=diff)) +
geom_point() + geom_line() + geom_smooth() +
ggtitle("relative gap in estimates")
# However, as epsilon gets stricter, the estimates also become poorer,
# relative to the actual set means (0 and 0.01, respectively)
estimateframe = melt(runframe, measure.vars=c("mean_s", "mean_prime"), variable.name="set", value.name="mean_value")
ggplot(estimateframe, aes(x=epsilon, y=mean_value, color=set)) + geom_line() +
geom_hline(yintercept = 0.01) + scale_color_manual(values=c("#1b9e77", "#d95f02")) +
ggtitle("actual estimates")
devtools::install_github('dmlc/xgboost',subdir='R-package')
install.packages("~/Documents/mxnet/R-package/mxnet_0.5.tar.gz", repos = NULL, type = "source")
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("mxnet")
require(mlbench)
require(mxnet)
data(Sonar,package = "mlbench")
head(Sonar)
Sonar[,61] = as.numeric(Sonar[,61])-1
head(Sonar)
train.x = data.matrix(Sonar[train.ind,1:60])
train.ind=c(1:50,100:150)
train.x = data.matrix(Sonar[train.ind,1:60])
test.x = data.matrix(Sonar[-train.ind,1:60])
test.y = Sonar[-train.ind,61]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy,device = 'CPU')
train.x = data.matrix(Sonar[train.ind,1:60])
train.y = Sonar[train.ind,61]
test.x = data.matrix(Sonar[-train.ind,1:60])
test.y = Sonar[-train.ind,61]
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy,device = 'CPU')
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy,device = "CPU")
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
eval.metric=mx.metric.accuracy)
mx.ctx.default()
preds = predict(model, test.x)
pred.label = max.col(t(preds))-1
table(pred.label, test.y)
data(BostonHousing, package="mlbench")
head(BostonHousing)
train.ind = seq(1, 506, 3)
train.ind
train.x = data.matrix(BostonHousing[train.ind, -14])
train.y = BostonHousing[train.ind, 14]
test.x = data.matrix(BostonHousing[-train.ind, -14])
test.y = BostonHousing[-train.ind, 14]
head(BostonHousing)
head(BostonHousing)
data <- mx.symbol.Variable("data")
# A fully connected hidden layer
# data: input source
# num_hidden: number of neurons in this layer
fc1 <- mx.symbol.FullyConnected(data, num_hidden=1)
# Use linear regression for the output layer
lro <- mx.symbol.LinearRegressionOutput(fc1)
lro <- mx.symbol.LinearRegressionOutput(fc1)
model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
ctx=mx.cpu(), num.round=50, array.batch.size=20,
learning.rate=2e-6, momentum=0.9, eval.metric=mx.metric.rmse)
preds = predict(model, test.x)
## Auto detect layout of input matrix, use rowmajor..
sqrt(mean((preds-test.y)^2))
demo.metric.mae <- mx.metric.custom("mae", function(label, pred) {
res <- mean(abs(label-pred))
return(res)
})
model <- mx.model.FeedForward.create(lro, X=train.x, y=train.y,
ctx=mx.cpu(), num.round=50, array.batch.size=20,
learning.rate=2e-6, momentum=0.9, eval.metric=demo.metric.mae)
## Auto detect layout of input matrix, use rowmajor..
## Start training with 1 devices
## [1] Train-mae=13.1889538083225
## [2] Train-mae=9.81431959337658
## [3] Train-mae=9.21576419870059
require(mxnet)
require(data.table)
install.packages('caret')
setwd("./Documents/R/Rcode//data/")
train <- fread('data/train.csv', header=TRUE)
train <- fread('handdc_train.csv', header=TRUE)
test <- fread('handdc_test.csv', header=TRUE)
train <- data.matrix(train)
test <- data.matrix(test)
View(head(train))
train.x <- train[,-1]
train.y <- train[,1]
train.x <- t(train.x/255)
test <- t(test/255)
table(train.y)
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")
devices <- mx.cpu()
model <- mx.model.FeedForward.create(softmax, X=train.x, y=train.y,
ctx=devices, num.round=10, array.batch.size=100,
learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
initializer=mx.init.uniform(0.07),
epoch.end.callback=mx.callback.log.train.metric(100))
preds <- predict(model, test)
dim(preds)
pred.label <- max.col(t(preds)) - 1
table(pred.label)
submission <- data.frame(ImageId=1:ncol(test), Label=pred.label)
data <- mx.symbol.Variable('data')
# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
mx.gpu(i)
})
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.cpu, num.round=1, array.batch.size=100,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.cpu, num.round=1, array.batch.size=100,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
require(mxnet)
require(imager)
install.packages("imager")
require(imager)
install.packages("~/Downloads/imager_0.14.tar.gz", repos = NULL, type = "source")
library(ggmap)
library(ggplot2)
model = mx.model.load("Inception/Inception_BN", iteration=39)
mean.img = as.array(mx.nd.load("Inception/mean_224.nd")[["mean_img"]])
mean.img
im <- load.image("pics/dog.jpeg")
install.packages("imager")
require(imager)
im <- load.image("pics/dog.jpeg")
im <- load.image("./pics/dog.jpeg")
getwd()
im <- load.image("../pics/dog.jpeg")
plot(im)
im <- load.image("../pics/dog.jpeg")
im <- load.image("../pics/dog.jpeg")
plot(im)
getwd()
im <- load.image("../pics/dog.jpg")
im <- load.image("../pics/Baker_Lake.jpg")
install.packages("gbm")
library("gbm", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("igraph")
require(igraph)
require(igraph)
install.packages("igraph")
require(igraph)
G <- graph( c(1,2,1,3,1,4,3,4,3,5,5,6,6,7,7,8,8,9,3,8,5,8), directed = FALSE )
# Assign attributes to the graph
G$name    <- "A colorful example graph"
# Assign attributes to the graph's vertices
V(G)$name  <- toupper(letters[1:9])
V(G)$color <- sample(rainbow(9),9,replace=FALSE)
# Assign attributes to the edges
E(G)$weight <- runif(length(E(G)),.5,4)
# Plot the graph -- details in the "Drawing graphs" section of the igraph manual
plot(G, layout = layout.fruchterman.reingold,
main = G$name,
vertex.label = V(G)$name,
vertex.size = 25,
vertex.color= V(G)$color,
vertex.frame.color= "white",
vertex.label.color = "white",
vertex.label.family = "sans",
edge.width=E(G)$weight,
edge.color="black")
E(G)$weight
E(G)$weight
G
length(E(G)
runif(length(E(G)),.5,4)
length(E(G))
G
G$name
V(G)
V(G)$color
sample(rainbow(9),9,replace=FALSE)
rainbow(9)
